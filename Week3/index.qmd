---
title: "Time Series Analysis"
subtitle: ""
author: "Dr. Thiyanga S. Talagala <br/>  Department of Statistics, Faculty of Applied Sciences <br/> University of Sri Jayewardenepura, Sri Lanka"
format:
  revealjs:
    css:
        - "custom.css"
    width: 1600
    height: 900
    margin: 0.1
    theme: simple
    transition: slide
    background-transition: fade
    slide-number: true
    show-slide-number: all
    title-slide-attributes: 
      data-background-color: "#081d58"
      data-background-image: none
---

```{r, comment=NA, echo=FALSE}
library(forecast)
library(fpp2)
```

## Identify the features of the time series

::::: columns
::: {.column width="50%"}
```{r}
#| warning: false
#| echo: false
library(forecast)
set.seed(20205)
ts.wn <- as.ts(rnorm(20))
autoplot(ts.wn)

```
:::

::: {.column width="50%"}
```{r}
ggAcf(ts.wn)
```
:::
:::::

## ACF plot

::::: columns
::: {.column width="50%"}
```{r}
library(forecast)
set.seed(20205)
ts.wn <- as.ts(rnorm(20))
autoplot(ts.wn)

```
:::

::: {.column width="50%"}
```{r}
ggAcf(ts.wn)
```
:::
:::::

White noise implies stationarity.

## Series with trend

::::: columns
::: {.column width="50%"}
```{r}
library(fpp2)
autoplot(uschange[,"Consumption"], main="Changes in US consumption and income")

```
:::

::: {.column width="50%"}
```{r}
ggAcf(uschange[,"Consumption"])
```
:::
:::::

## Common trend removal (de-trending) procedures

1.  Deterministic trend: Time-trend regression

    The trend can be removed by fitting a deterministic polynomial time trend. The residual series after removing the trend will give us the de-trended series.

2.  Stochastic trend: Differencing

    The process is also known as a **Difference-stationary process**.

------------------------------------------------------------------------

## Notation: I(d)

Integrated to order $d$: Series can be made stationary by differencing $d$ times.

-   Known as $I(d)$ process.

**Question:** Show that random walk process is an $I(1)$ process.

The random walk process is called a unit root process. (If one of the roots turns out to be one, then the process is called unit root process.)

# Variance stabilization

Eg:

-   Square root: $W_t = \sqrt{Y_t}$

-   Logarithm: $W_t = log({Y_t})$

    -   This very useful.

    -   Interpretable: Changes in a log value are **relative (percent) changes on the original sclae**.

## Monthly Airline Passenger Numbers 1949-1960

**Without transformations**

```{r }
#| echo: FALSE
#| warning: FALSE
#| message: FALSE
data(elec)
autoplot(AirPassengers)+ylab("Monthly Airline Passenger Numbers 1949-1960")
```

## Monthly Airline Passenger Numbers 1949-1960

**Logarithm transformation**

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(forecast)
library(fpp2)
data(AirPassengers)
autoplot(log(AirPassengers))+ylab("log(Monthly Airline Passenger Numbers 1949-1960)")
```

# Box-Cox transformation

$$
  w_t=\begin{cases}
    log(y_t), & \text{if $\lambda=0$} \newline
    (Y_t^\lambda - 1)/ \lambda, & \text{otherwise}.
  \end{cases}
$$

Different values of $\lambda$ gives you different transformations.

-   $\lambda=1$: No **substantive** transformation

-   $\lambda = \frac{1}{2}$: Square root plus linear transformation

-   $\lambda=0$: Natural logarithm

-   $\lambda = -1$: Inverse plus 1

Balance the seasonal fluctuations and random variation across the series.

# Transformation often makes little difference to forecasts but has large effects on PI.

# Application

`snaive` + applying BoxCox transformation

```{r}
fit <- snaive(AirPassengers, lambda = 0)
autoplot(fit)
```

# Is the snaive method suitable for forecasting the AirPassengers time series?

# Steps:

✅ apply Box-Cox transformation.

✅ fit a model.

✅ reverse transformation.

# What differences do you notice?

::::: columns
::: {.column width="50%"}
```{r, comment=NA, message=FALSE, warning=FALSE, fig.height=6, echo=FALSE}
autoplot(AirPassengers)
```
:::

::: {.column width="50%"}
```{r, comment=NA, message=FALSE, warning=FALSE, fig.height=6, echo=FALSE}
autoplot(cangas)
```
:::
:::::

<!--Monotonically increasing variance vs Non-monotonically increasing variance. Any monotonic transformation wouldn't work here. When you apply boxcox transformation it will transform one part and do the opposite for the other part. There are different ways to handle this. What transformation would work for cangas data set. Video:sd1-4 (48) -->

# ARMA(p, q) model

$$Y_t=c+\phi_1Y_{t-1}+...+\phi_p Y_{t-p}+ \theta_1\epsilon_{t-1}+...+\theta_q\epsilon_{t-q}+\epsilon_t$$

-   These are stationary models.

-   They are only suitable for **stationary series**.

# ARIMA(p, d, q) model

Differencing --\> ARMA

**Step 1: Differencing**

$$Y'_t = (1-B)^dY_t$$

**Step 2: ARMA**

$$Y'_t=c+\phi_1Y'_{t-1}+...+\phi_p Y'_{t-p}+ \theta_1\epsilon_{t-1}+...+\theta_q\epsilon_{t-q}+\epsilon_t$$

## Modelling steps

1.  Plot the data.

2.  Split time series into training, validation (optional), test.

3.  If necessary, transform the data (using a Box-Cox transformation) to stabilise the variance.

4.  If the data are non-stationary, take first differences of the data until the data are stationary.

5.  Examine the ACF/PACF to identify a suitable model.

6.  Try your chosen model(s), and to search for a better model.

7.  Check the residuals from your chosen model by plotting the ACF of the residuals, and doing a portmanteau test of the residuals. If they do not look like white noise, try a modified model.

8.  Once the residuals look like white noise, calculate forecasts.

# Step 1: Plot data

# 

1.  Detect unusual observations in the data

2.  Detect non-stationarity by visual inspections of plots

Stationary series:

-   has a constant mean value and fluctuates around the mean.

-   constant variance.

-   no pattern predictable in the long-term.

# Step 2: Split time series into training and test sets

```{r}
training.ap <- window(AirPassengers, end=c(1957, 12))
training.ap
test.ap <- window(AirPassengers, start=c(1958, 1))
test.ap
```

# 

```{r}
autoplot(AirPassengers) + 
  geom_vline(xintercept = 1958, colour="forestgreen")

```

# Training part

```{r}
autoplot(training.ap)
```

# Step 3: Apply transformations

```{r}
log.airpassenger <- log(training.ap)
#log.airpassenger <- BoxCox(training.ap, lambda = 0)
autoplot(log.airpassenger)

```

## Step 4: Take difference series

### Identifying non-stationarity by looking at plots

-   Time series plot

-   The ACF of stationary data drops to zero relatively quickly.

-   The ACF of non-stationary data decreases slowly.

-   For non-stationary data, the value of $r_1$ is often large and positive.

## Non-seasonal differencing and seasonal differencing

**Non seasonal first-order differencing:** $Y'_t=Y_t - Y_{t-1}$

<!--Miss one observation-->

**Non seasonal second-order differencing:** $Y''_t=Y'_t - Y'_{t-1}$

<!--Miss two observations-->

**Seasonal differencing:** $Y_t - Y_{t-m}$

<!--To get rid from prominent seasonal components. -->

-   For monthly, $m=12$, for quarterly, $m=4$.

<!--We will loosefirst 12 observations-->

-   Seasonally differenced series will have $T-m$ observations. <!--Usually we do not consider differencing more than twice. -->

> There are times differencing once is not enough. However, in practice,it is almost never necessary to go beyond second-order differencing.

<!--Even the second-order differencing is very rare.-->

## Non-Seasonal differencing

### Without differencing

::::: columns
::: {.column width="50%"}
```{r}

autoplot(log.airpassenger)

```
:::

::: {.column width="50%"}
```{r, comment=NA, warning=FALSE, message=FALSE}

ggAcf(log.airpassenger)

```
:::
:::::

### With differencing

::::: columns
::: {.column width="50%"}
```{r}

log.airpassenger |> diff(lag=12)  |> autoplot()

```
:::

::: {.column width="50%"}
```{r}

log.airpassenger |> diff(lag=1)  |> ggAcf()

```
:::
:::::

## Seasonal differencing

::::: columns
::: {.column width="50%"}
```{r}

log.airpassenger |> diff(lag=1)  |> autoplot()

```
:::

::: {.column width="50%"}
```{r}

log.airpassenger |> diff(lag=12)  |> ggAcf()

```
:::
:::::

## Seasonal differencing + Non-seasonal differencing

::::: columns
::: {.column width="50%"}
```{r}

log.airpassenger |> diff(lag=12)  |> diff(lag=1) |> autoplot()

```
:::

::: {.column width="50%"}
```{r}

log.airpassenger |> diff(lag=12)  |> 
  diff(lag=1) |> ggAcf()

```
:::
:::::

# Testing for nonstationarity for the presence of unit roots

-   Dickey and Fuller (DF) test

-   Augmented DF test

-   Phillips and Perron (PP) nonparametric test

-   Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test

## KPSS test

**H0:** Series is level or trend stationary.

**H1:** Series is not stationary.

```{r}
library(urca)
diff.sdiff.log.passenger <- log.airpassenger |>
  diff(lag=12) |>
  diff(lag=1)

diff.sdiff.log.passenger |>
  ur.kpss() |>
  summary()
```

## KPSS test

```{r}
ur.kpss(log.airpassenger) |> summary()
```

```{r}
sdiff.log.airpassenger <- training.ap |> log() |> diff(lag=12)
ur.kpss(sdiff.log.airpassenger) |> summary()
```

## AR(p)

-   ACF dies out in an exponential or damped sine-wave manner.

-   there is a significant spike at lag $p$ in PACF, but none beyond $p$.

## MA(q)

-   ACF has all zero spikes beyond the $q^{th}$ spike.

-   PACF dies out in an exponential or damped sine-wave manner.

## Seasonal components

-   The seasonal part of an AR or MA model will be seen in the seasonal lags of the PACF and ACF.

::::

::: {.column width="50%"}
**ARIMA(0,0,0)(0,0,1)12 will show**

-   a spike at lag 12 in the ACF but no other significant spikes.

-   The PACF will show exponential decay in the seasonal lags 12, 24, 36, . . . .
:::

::: {.column width="50%"}
**ARIMA(0,0,0)(1,0,0)12 will show**

-   exponential decay in the seasonal lags of the ACF.

-   a single significant spike at lag 12 in the PACF.
:::

::::

## ACF

```{r}
log.airpassenger |> diff(lag=12)  |> 
  diff(lag=1) |> ggAcf()
```

## PACF

```{r}
log.airpassenger |> diff(lag=12)  |> 
  diff(lag=1) |> ggPacf()
```

## Step 5: Examine the ACF/PACF to identify a suitable model

-   $d=1$ and $D=1$ (from step 3)

-   Significant spike at lag 1 in ACF suggests non-seasonal MA(1) component.

-   Significant spike at lag 12 in ACF suggests seasonal MA(1) component.

-   Initial candidate model: $ARIMA(0,1,1)(0,1,1)_{12}$.

-   By analogous logic applied to the PACF, we could also have started with $ARIMA(1,1,0)(1,1,0)_{12}$.

-   Let's try both

## 

**Initial model:**

$ARIMA(0,1,1)(0,1,1)_{12}$

$ARIMA(1,1,0)(1,1,0)_{12}$

**Try some variations of the initial model:**

$ARIMA(0,1,1)(1,1,1)_{12}$

$ARIMA(1,1,1)(1,1,0)_{12}$

$ARIMA(1,1,1)(1,1,1)_{12}$

Both the ACF and PACF show significant spikes at lag 3, and almost significant spikes at lag 3, indicating that some additional non-seasonal terms need to be included in the model.

$ARIMA(3,1,1)(1,1,1)_{12}$

$ARIMA(1,1,3)(1,1,1)_{12}$

$ARIMA(3,1,3)(1,1,1)_{12}$

## AICc

**Initial model: AICc**

$ARIMA(0,1,1)(0,1,1)_{12}$: -344.33 (the smallest AICc)

$ARIMA(1,1,0)(1,1,0)_{12}$: -336.32

**Try some variations of the initial model:**

$ARIMA(0,1,1)(1,1,1)_{12}$: -342.3 (second smallest AICc)

$ARIMA(1,1,1)(1,1,0)_{12}$: -336.08

$ARIMA(1,1,1)(1,1,1)_{12}$: -340.74

$ARIMA(3,1,1)(1,1,1)_{12}$: -338.89

$ARIMA(1,1,3)(1,1,1)_{12}$: -339.42

$ARIMA(3,1,3)(1,1,1)_{12}$: -335.65

## Step 7: Residual diagnostics

### Fitted values:

$\hat{Y}_{t|t-1}$: Forecast of $Y_t$ based on observations $Y_1,...Y_t$.

### Residuals

$$e_t=Y_t - \hat{Y}_{t|t-1}$$

### Assumptions of residuals

-   $\{e_t\}$ uncorrelated. If they aren't, then information left in residuals that should be used in computing forecasts.

<!--If you see autocorrelations, then you should go back and adjust residuals. In theoretically, If there is information leftover and we can do something better. But it is not the case you will also be able to do with. If can't you can't. Then stop. If you check you know you have done the best as you can.-->

-   $\{e_t\}$ have mean zero. If they don't, then forecasts are biased.

<!--If you see autocorrelations, then you should go back and adjust residuals. We want our residuals to be unbiased. If the mean is not zero. Go and adjust the model. Add an intercept. Whatever you want to do.-->

## Useful properties (for prediction intervals)

-   $\{e_t\}$ have constant variance.

-   $\{e_t\}$ are normally distributed.

<!--If the following assumptions are wrong that doesn't mean your forecasts are incorrect. -->

## Step 7: Residual diagnostics (cont.)

H0: Data are not serially correlated.

H1: Data are serially correlated.

```{r, comment=NA, warning=FALSE, message=FALSE,fig.height=3}
fit1 <- Arima(training.ap, 
              order=c(0,1,1),
seasonal=c(0,1,1), lambda = 0)
checkresiduals(fit1)
```

## Step 7: Residual diagnostics (cont.)

```{r, comment=NA, warning=FALSE, message=FALSE}
fit1 %>% residuals() %>% ggtsdisplay()
```

## Step 7: Residual diagnostics (cont.)

```{r, comment=NA, warning=FALSE, message=FALSE, fig.height=4}
fit3 <- Arima(training.ap, 
              order=c(0,1,1),
seasonal=c(1,1,1), lambda = 0)
checkresiduals(fit3)
```

## Step 7: Residual diagnostics (cont.)

```{r, comment=NA, warning=FALSE, message=FALSE}
fit3 %>% residuals() %>% ggtsdisplay()
```

## Step 8: Calculate forecasts

$ARIMA(0,1,1)(0,1,1)_{12}$

```{r, comment=NA, warning=FALSE, message=FALSE}
fit1 %>% forecast(h=36) %>% 
  autoplot()

```

## 

$ARIMA(0,1,1)(1,1,1)_{12}$

```{r, comment=NA, warning=FALSE, message=FALSE}
fit3 %>% forecast(h=36) %>% 
  autoplot()

```

## 

$ARIMA(0,1,1)(0,1,1)_{12}$

```{r, comment=NA, warning=FALSE, message=FALSE}
fit1.forecast <- fit1 %>% forecast(h=36) 
fit1.forecast$mean

```

$ARIMA(0,1,1)(1,1,1)_{12}$

```{r, comment=NA, warning=FALSE, message=FALSE}
fit3.forecast <- fit3 %>% forecast(h=36) 
fit3.forecast$mean
```

## Step 9: Evaluate forecast accuracy

### How well our model is doing for out-of-sample?

<!--So far we have talked about fitted values and residuals.-->

<!--Train data and Test data. We want to know if forecasts doing well for out-of-sample.-->

Forecast error = True value - Observed value

$$e_{T+h}=Y_{T+h}-\hat{Y}_{T+h|T}$$

Where,

$Y_{T+h}$: $(T+h)^{th}$ observation, $h=1,..., H$

$\hat{Y}_{T+h|T}$: Forecast based on data uo to time $T$.

-   **True** forecast error as the test data is not used in computing $\hat{Y}_{T+h|T}$.

-   Unlike, residuals, forecast errors on the test set involve multi-step forecasts.

-   Use forecast error measures to evaluate the models.

<!--Since, true forecast error, no hat involved.-->

## Step 9: Evaluate forecast accuracy

$ARIMA(0,1,1)(0,1,1)_{12}$

```{r, comment=NA, warning=FALSE, message=FALSE}



fit1.forecast <- fit1 |> 
  forecast(h=36) 

```

```{r, comment=NA}
accuracy(fit1.forecast$mean, test.ap)
```

$ARIMA(0,1,1)(1,1,1)_{12}$

```{r, comment=NA, warning=FALSE, message=FALSE}

fit3.forecast <- fit3 |>
  forecast(h=36) 

```

```{r, comment=NA}
accuracy(fit3.forecast$mean, test.ap)
```

$ARIMA(0,1,1)(0,1,1)_{12}$ MAE, MAPE is smaller than $ARIMA(0,1,1)(1,1,1)_{12}$. Hence, we select $ARIMA(0,1,1)(0,1,1)_{12}$ to forecast future values.
